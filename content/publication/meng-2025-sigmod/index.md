---
title: "Revisiting Graph Analytics Benchmark"
date: 2025-04-01
publishDate: 2025-04-01T01:41:34.981585Z
authors: ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Xue Li", "Wenyuan Yu","Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"]
publication_types: ["1"]
abstract: The rise of graph analytics platforms has led to the development of various benchmarks for evaluating and comparing platform performance. However, existing benchmarks often fall short of fully assessing performance due to limitations in core algorithm selection, data generation processes (and the corresponding synthetic datasets), as well as the neglect of API usability evaluation. To address these shortcomings, we propose a novel graph analytics benchmark. First, we select eight core algorithms by extensively reviewing both academic and industrial settings. Second, we design an efficient and flexible data generator and produce eight new synthetic datasets as the default datasets for our benchmark. Lastly, we introduce a multi-level large language model (LLM)-based framework for API usability evaluation-the first of its kind in graph analytics benchmarks. We conduct comprehensive experimental evaluations on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra, and G-thinker). The experimental results demonstrate the superiority of our proposed benchmark.
featured: true
url_pdf: files/ganalytics-sigmod.pdf
publication: "ACM SIGMOD/PODS International Conference on Management of Data 2025 (to appear)"
doi: "10.1145/3694966"
tags: ["Graph Algorithm", "Distributed Processing", "Comprehensive Benchmark", "LLM-based Evaluation"]
---

